{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up executable path for chromedriver\n",
    "executable_path = {'executable_path':'chromedriver'}\n",
    "\n",
    "# URL of page to be scraped for news, images, weather and facts\n",
    "url_news = 'https://mars.nasa.gov/news/'\n",
    "url_featured_images = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "url_weather = 'https://twitter.com/marswxreport?lang=en'\n",
    "url_facts = 'http://space-facts.com/mars/'\n",
    "url_hemisperes_images = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use splinter module to retrieve data from websites\n",
    "browser = Browser('chrome', **executable_path,headless = True)\n",
    "browser.visit(url_news)\n",
    "html = browser.html\n",
    "# create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html,'html.parser')\n",
    "# save news title and paragraph to variables\n",
    "news_title = soup.find('div',class_=\"content_title\").a.text.strip()\n",
    "news_p = soup.find('div',class_=\"rollover_description_inner\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser.visit(url_featured_images)\n",
    "html = browser.html\n",
    "# create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html,'html.parser')\n",
    "# retrieve the data link to the website which has full size image\n",
    "url_imagelink = soup.find('footer').a['data-link']\n",
    "url_imagelink = 'https://www.jpl.nasa.gov' + url_imagelink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visit the linked website and retrieve the url of full size image\n",
    "browser.visit(url_imagelink)\n",
    "html = browser.html\n",
    "# create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html,'html.parser')\n",
    "# save the url of featured image to variables\n",
    "featured_image_url = soup.find('figure',class_='lede').a['href']\n",
    "featured_image_url = 'https://www.jpl.nasa.gov' + featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(url_weather)\n",
    "html = browser.html\n",
    "# create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html,'html.parser')\n",
    "# save mars weather to variables\n",
    "Mars_weather = soup.find('div',class_='js-tweet-text-container').p.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve Mars fact table and read into pandas dataframe\n",
    "df = pd.read_html(url_facts)\n",
    "facts_table = df[0]\n",
    "# add column headers and set index\n",
    "facts_table.columns = ['description','value']\n",
    "facts_table.set_index('description',inplace=True)\n",
    "# save to html tables\n",
    "Mars_fact = facts_table.to_html(index=False,justify='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve Mars hemisperes images and save to a list of dictionaries\n",
    "browser.visit(url_hemisperes_images)\n",
    "html = browser.html\n",
    "# create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html,'html.parser')\n",
    "# create list and dictionary to capture titles and image urls\n",
    "img_list = []\n",
    "img_dict = {}\n",
    "results = soup.find_all('div',class_='item')\n",
    "# loop through all scraped items and add them to the dictionary, then add to the list\n",
    "for result in results:\n",
    "    title = result.find('div',class_='description').a.text.strip()\n",
    "    img_link = 'https://astrogeology.usgs.gov' + result.find('div',class_='description').a['href']\n",
    "    browser.visit(img_link)\n",
    "    html = browser.html\n",
    "    soup = bs(html,'html.parser')\n",
    "    hemisperes_images_url = 'https://astrogeology.usgs.gov' + soup.find('img',class_='wide-image')['src']\n",
    "    img_dict = {'title':title,'img_url':hemisperes_images_url}\n",
    "    img_list.append(img_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
